{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Legal-BERT Fine-Tuning\n",
        "This notebook fine-tunes Legal-BERT for clause classification using the LegalBench CUAD dataset."
      ],
      "metadata": {
        "id": "Jhsv0bPRcHKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xx2HQZPLbjGI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess Dataset"
      ],
      "metadata": {
        "id": "_4LwejuRcQjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset_cap = load_dataset(\"nguha/legalbench\", \"cuad_cap_on_liability\", trust_remote_code=True)\n",
        "dataset_audit = load_dataset(\"nguha/legalbench\", \"cuad_audit_rights\", trust_remote_code=True)\n",
        "dataset_insurance = load_dataset(\"nguha/legalbench\", \"cuad_insurance\", trust_remote_code=True)\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_cap = pd.DataFrame(dataset_cap['test'])\n",
        "df_audit = pd.DataFrame(dataset_audit['test'])\n",
        "df_insurance = pd.DataFrame(dataset_insurance['test'])\n",
        "\n",
        "# Filter 'yes' examples\n",
        "df_cap_yes = df_cap[df_cap['answer'].str.lower() == 'yes'].copy()\n",
        "df_cap_yes.loc[:, 'label'] = 0  # cap_on_liability\n",
        "df_audit_yes = df_audit[df_audit['answer'].str.lower() == 'yes'].copy()\n",
        "df_audit_yes.loc[:, 'label'] = 1  # audit_rights\n",
        "df_insurance_yes = df_insurance[df_insurance['answer'].str.lower() == 'yes'].copy()\n",
        "df_insurance_yes.loc[:, 'label'] = 2  # insurance\n",
        "\n",
        "# Filter 'no' examples and sample a subset (20% of 'yes' count)\n",
        "df_cap_no = df_cap[df_cap['answer'].str.lower() == 'no'].sample(n=int(0.2 * len(df_cap_yes)), random_state=42).copy()\n",
        "df_cap_no.loc[:, 'label'] = 3  # 'no' class\n",
        "df_audit_no = df_audit[df_audit['answer'].str.lower() == 'no'].sample(n=int(0.2 * len(df_audit_yes)), random_state=42).copy()\n",
        "df_audit_no.loc[:, 'label'] = 3  # 'no' class\n",
        "df_insurance_no = df_insurance[df_insurance['answer'].str.lower() == 'no'].sample(n=int(0.2 * len(df_insurance_yes)), random_state=42).copy()\n",
        "df_insurance_no.loc[:, 'label'] = 3  # 'no' class\n",
        "\n",
        "# Combine all\n",
        "df_combined = pd.concat([df_cap_yes, df_audit_yes, df_insurance_yes, df_cap_no, df_audit_no, df_insurance_no], ignore_index=True)\n",
        "\n",
        "# Check for duplicates\n",
        "print(f\"Total clauses before deduplication: {len(df_combined)}\")\n",
        "df_combined = df_combined.drop_duplicates(subset=['text'], keep='first')\n",
        "print(f\"Total clauses after deduplication: {len(df_combined)}\")\n",
        "\n",
        "# Preprocess text\n",
        "df_combined.loc[:, 'cleaned_text'] = df_combined['text'].apply(lambda x: x.strip().lower())  # Fixed with .loc\n",
        "\n",
        "# Split data\n",
        "train_data, test_data = train_test_split(df_combined, test_size=0.2, stratify=df_combined['label'], random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_data['cleaned_text'], train_data['label'], test_size=0.2, stratify=train_data['label'], random_state=42\n",
        ")\n",
        "print(f\"Training set size: {len(train_texts)}, Validation set size: {len(val_texts)}, Test set size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "5nCylhsScNcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "523c76b8-5587-4d9d-bdb2-890da14588cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total clauses before deduplication: 2094\n",
            "Total clauses after deduplication: 2052\n",
            "Training set size: 1312, Validation set size: 329, Test set size: 411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Tokenizer and Model"
      ],
      "metadata": {
        "id": "kgoT6BEycVMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=4)  # Now 4 classes\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
        "train_labels = torch.tensor(train_labels.tolist())\n",
        "val_labels = torch.tensor(val_labels.tolist())"
      ],
      "metadata": {
        "id": "ey6AwSBLcWrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266196b9-624a-4bff-edfb-f4ec86788d3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Class and Instances"
      ],
      "metadata": {
        "id": "03VqkPiVcavi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class LegalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = LegalDataset(train_encodings, train_labels)\n",
        "val_dataset = LegalDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "8209XkS6ccgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluation"
      ],
      "metadata": {
        "id": "_QMoJ80CcfQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy='epoch',\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = predictions.label_ids\n",
        "print(f'Accuracy: {accuracy_score(labels, preds):.4f}, '\n",
        "      f'Precision: {precision_score(labels, preds, average=\"macro\"):.4f}, '\n",
        "      f'Recall: {recall_score(labels, preds, average=\"macro\"):.4f}, '\n",
        "      f'F1: {f1_score(labels, preds, average=\"macro\"):.4f}')"
      ],
      "metadata": {
        "id": "inH85uD9cgzk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b4715214-d540-4207-e1af-df6de368eaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='984' max='984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [984/984 07:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.135700</td>\n",
              "      <td>0.130547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.193639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.159535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9787, Precision: 0.9786, Recall: 0.9721, F1: 0.9751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model"
      ],
      "metadata": {
        "id": "fwv6WuoPckj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save_pretrained('fine-tuned-legal-bert')\n",
        "tokenizer.save_pretrained('fine-tuned-legal-bert')"
      ],
      "metadata": {
        "id": "QTl3mqHjcl9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c086a6c-0638-41e5-8d8f-508940bbccca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine-tuned-legal-bert/tokenizer_config.json',\n",
              " 'fine-tuned-legal-bert/special_tokens_map.json',\n",
              " 'fine-tuned-legal-bert/vocab.txt',\n",
              " 'fine-tuned-legal-bert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}